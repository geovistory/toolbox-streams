# App settings
create.output.for.postgres=false
ts.input.topic.name.prefix=
ts.output.topic.name.prefix=
ts.community.slug=toolbox
prefix=${ts.input.topic.name.prefix}
ts.output.topic.partitions=4
ts.output.topic.replication.factor=3
# set this to "disabled" to skip output topic creation, e.g. for tests with test driver
auto.create.output.topics=enabled
# Input topics
ts.topic.ontome.class.metadata=my.app.ontome.class.metadata
ts.topic.has.type.property=my.app.has.type.property
ts.topic.project.entity=my.app.project.entity
ts.topic.project.top.outgoing.statements=my.app.project.top.outgoing.statements
ts.topic.project.class.label=my.app.project.class.label
ts.topic.community.entity=my.app.community.entity
ts.topic.community.top.outgoing.statements=my.app.community.top.outgoing.statements
ts.topic.community.class.label=my.app.community.class.label
# Quarkus kafka streams settings
quarkus.kafka-streams.application-id=my-app
kafka.bootstrap.servers=localhost:9092
schema.registry.url=foo
quarkus.kafka-streams.topics=\
  ${ts.topic.ontome.class.metadata},\
  ${ts.topic.has.type.property},\
  ${ts.topic.project.entity},\
  ${ts.topic.project.top.outgoing.statements},\
  ${ts.topic.project.class.label},\
  ${ts.topic.community.entity},\
  ${ts.topic.community.top.outgoing.statements},\
  ${ts.topic.community.class.label}

# kafka-streams pass-through options
## general
kafka-streams.state.dir=kafka_streams_state
kafka-streams.topology.optimization=all
kafka-streams.processing.guarantee=exactly_once_v2

# streams
kafka-streams.cache.max.bytes.buffering=10240
kafka-streams.commit.interval.ms=100
kafka-streams.metadata.max.age.ms=500
kafka-streams.buffered.records.per.partition=1000

# producer
kafka-streams.producer.max.request.size=20971760
kafka-streams.producer.buffer.memory=33554432
kafka-streams.producer.send.buffer.bytes=131072
kafka-streams.producer.interceptor.classes=
kafka-streams.producer.transaction.timeout.ms=300000


#consumer
kafka-streams.consumer.isolation.level=read_committed
kafka-streams.consumer.fetch.max.bytes=52428800
kafka-streams.consumer.fetch.max.wait.ms=500
kafka-streams.consumer.receive.buffer.bytes=32768
kafka-streams.consumer.interceptor.classes=

# debug / metrics configs
kafka-streams.metrics.recording.level=INFO
management.metrics.tags.application=${quarkus.kafka-streams.application-id}
# Build settings
quarkus.native.container-build=true
# Docker settings
quarkus.container-image.group=geovistory
quarkus.container-image.registry=ghcr.io
quarkus.container-image.push=true
# disable launching of apicurio-registry container in devservices (dev/test mode)
quarkus.apicurio-registry.devservices.enabled=false
# Dev mode
%dev.kafka.bootstrap.servers=localhost:1121,localhost:1122,localhost:1123
%dev.schema.registry.url=http://localhost:1127
%dev.kafka-streams.state.dir=state_dev_mode
# Test mode
%test.kafka-streams.state.dir=state_test_mode
%test.kafka-streams.consumer.session.timeout.ms=6000
%test.kafka-streams.consumer.heartbeat.interval.ms=5500
%test.ts.output.topic.replication.factor=1
quarkus.test.continuous-testing=enabled
quarkus.test.flat-class-path=true
# disable launching of kafka broker, since we set up redpanda ourselves
%test.quarkus.kafka.devservices.enabled=false