/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package org.geovistory.toolbox.streams.project.entity;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.config.TopicConfig;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.geovistory.toolbox.streams.lib.Admin;
import org.geovistory.toolbox.streams.lib.AppConfig;
import org.geovistory.toolbox.streams.lib.BoundedMemoryRocksDBConfig;
import org.geovistory.toolbox.streams.project.entity.topologies.*;

import java.util.Properties;

class App {
    public static void main(String[] args) {


        StreamsBuilder builder = new StreamsBuilder();

        // add processors of sub-topologies
        addSubTopologies(builder);

        // build the topology
        var topology = builder.build();

        System.out.println(topology.describe());

        // create topics in advance to ensure correct configuration (partition, compaction, ect.)
        createTopics();

        // print configuration information
        System.out.println("Starting Toolbox Streams App v" + BuildProperties.getDockerTagSuffix());
        System.out.println("With config:");
        AppConfig.INSTANCE.printConfigs();

        // create the streams app
        // noinspection resource
        KafkaStreams streams = new KafkaStreams(topology, getConfig());

        // close Kafka Streams when the JVM shuts down (e.g. SIGTERM)
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

        // start streaming!
        streams.start();
    }

    private static void addSubTopologies(StreamsBuilder builder) {
        var inputTopic = new RegisterInputTopic(builder);

        // register input topics as KTables
        var projectEntityLabelConfigTable = inputTopic.projectEntityLabelConfigTable();
        var projectEntityTable = inputTopic.projectEntityTable();
        var projectTopOutgoingStatementsTable = inputTopic.projectTopOutgoingStatementsTable();
        var hasTypePropertyTable = inputTopic.hasTypePropertyTable();
        var ontomeClassMetadataTable = inputTopic.ontomeClassMetadataTable();

        // register input topics as KStreams
        var projectEntityTopStatementsStream = inputTopic.projectEntityTopStatementsStream();
        var projectClassLabelTable = inputTopic.projectClassLabelTable();
        var projectEntityTopStatementsTable = inputTopic.projectEntityTopStatementsTable();


        // add sub-topology ProjectEntityFulltext
        ProjectEntityFulltext.addProcessors(builder,
                projectEntityTopStatementsTable,
                projectEntityLabelConfigTable
        );

        // add sub-topology ProjectEntityTimeSpan
        ProjectEntityTimeSpan.addProcessors(builder,
                projectEntityTopStatementsStream
        );

        // add sub-topology ProjectEntityType
        ProjectEntityType.addProcessors(builder,
                projectEntityTable,
                hasTypePropertyTable,
                projectTopOutgoingStatementsTable
        );

        // add sub-topology ProjectEntityClassLabel
        ProjectEntityClassLabel.addProcessors(builder,
                projectEntityTable,
                projectClassLabelTable
        );

        // add sub-topology ProjectEntityClassMetadata
        ProjectEntityClassMetadata.addProcessors(builder,
                projectEntityTable,
                ontomeClassMetadataTable
        );

    }

    private static void createTopics() {
        var admin = new Admin();

        var outputTopicPartitions = Integer.parseInt(AppConfig.INSTANCE.getOutputTopicPartitions());
        var outputTopicReplicationFactor = Short.parseShort(AppConfig.INSTANCE.getOutputTopicReplicationFactor());

        // create output topics (with number of partitions and delete.policy=compact)
        admin.createOrConfigureTopics(new String[]{
                ProjectEntityClassLabel.output.TOPICS.project_entity_class_label,
                ProjectEntityFulltext.output.TOPICS.project_entity_fulltext,
                ProjectEntityTimeSpan.output.TOPICS.project_entity_time_span,
                ProjectEntityClassMetadata.output.TOPICS.project_entity_class_metadata,
                ProjectEntityType.output.TOPICS.project_entity_type,
        }, outputTopicPartitions, outputTopicReplicationFactor);


       /* // Use the KafkaFuture object to block and wait for the topic creation to complete
        try {
            kafkaFuture.get();
            System.out.println("Topics created successfully");
        } catch (InterruptedException | ExecutionException e) {
            System.out.println("Error creating topics: " + e.getMessage());
        }*/
    }

    private static Properties getConfig() {

        AppConfig appConfig = AppConfig.INSTANCE;

        // set the required properties for running Kafka Streams
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, appConfig.getApplicationId());
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, appConfig.getKafkaBootstrapServers());
        props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, StreamsConfig.OPTIMIZE);

        props.put(StreamsConfig.STATE_DIR_CONFIG, appConfig.getStateDir());

        props.put(StreamsConfig.topicPrefix(TopicConfig.CLEANUP_POLICY_CONFIG), TopicConfig.CLEANUP_POLICY_COMPACT);

        // See this for producer configs:
        // https://docs.confluent.io/platform/current/streams/developer-guide/config-streams.html#ak-consumers-producer-and-admin-client-configuration-parameters
        props.put(StreamsConfig.producerPrefix(ProducerConfig.MAX_REQUEST_SIZE_CONFIG), "20971760");

        // rocksdb memory management
        // see https://medium.com/@grinfeld_433/kafka-streams-and-rocksdb-in-the-space-time-continuum-and-a-little-bit-of-configuration-40edb5ee9ed7
        // see https://kafka.apache.org/33/documentation/streams/developer-guide/memory-mgmt.html#id3
        props.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, BoundedMemoryRocksDBConfig.class.getName());
        props.put(BoundedMemoryRocksDBConfig.TOTAL_OFF_HEAP_SIZE_MB, appConfig.getRocksdbTotalOffHeapMb());
        props.put(BoundedMemoryRocksDBConfig.TOTAL_MEMTABLE_MB, appConfig.getRocksdbTotalMemtableMb());

        // streams memory management
        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, appConfig.getStreamsCacheMaxBytesBuffering());
        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, appConfig.getStreamsCommitIntervalMs());
        props.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, appConfig.getStreamsBufferedRecordsPerPartition());

        // producer memory management
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, appConfig.getStreamsBufferMemory());
        props.put(ProducerConfig.SEND_BUFFER_CONFIG, appConfig.getStreamsSendBufferBytes());

        // consumer memory management
        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, appConfig.getStreamsFetchMaxBytes());
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, appConfig.getStreamsFetchMaxWaitMs());
        props.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, appConfig.getStreamsReceiveBufferBytes());


        /*

        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, AvroSerde.class);
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, AvroSerde.class);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        // URL for Apicurio Registry connection (including basic auth parameters)
        props.put(SerdeConfig.REGISTRY_URL, appConfig.getApicurioRegistryUrl());
        // Specify using specific (generated) Avro schema classes
        props.put(AvroKafkaSerdeConfig.USE_SPECIFIC_AVRO_READER, "true");
        props.put(SerdeConfig.AUTO_REGISTER_ARTIFACT, true);*/
        return props;
    }


}
